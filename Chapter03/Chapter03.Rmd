---
title: 'Introductory Statistics with Randomization and Simulation: Chapter 3'
output:
   ioslides_presentation:
     font-family: Lato Semibold
     font-import: http://fonts.googleapis.com/css?family=Lato
     widescreen: yes
     css: ../style.css
     fig_caption: yes
---
<style>
citation {
  font-size: 4px;
}
</style>

<!--  Version 1.0-0

      This version of the slides is taken directly from Mine Çetinkaya-Rundel's lecture slides
      posted on OpenIntro.org in .pptx and .gdslides format. Simply moved to Rmd. 
      
      A large part of the HTML/CSS formatting is janky, and could be cleaned up. Feel free to issue a 
      pull request if you love HTML and CSS and want to fix this up.
      
      - wburr, Sept 1, 2017
-->

<!-- This is Chapter 3.1 in the text, slides by Mine Cetinkaya-Rundel -->
# Inference for a Single Proportion

## Practice

Two scientists want to know if a certain drug is effective against high blood pressure. The first scientist wants to give the drug to 1000 people with high blood pressure and see how many of them experience lower blood pressure levels. The second scientist wants to give the drug to 500 people with high blood pressure, and not give the drug to another 500 people with high blood pressure, and see how many in both groups experience lower blood pressure levels. Which is the better way to test this drug?

1. All 1000 get the drug
2. 500 get the drug, 500 don’t

## Practice 

Two scientists want to know if a certain drug is effective against high blood pressure. The first scientist wants to give the drug to 1000 people with high blood pressure and see how many of them experience lower blood pressure levels. The second scientist wants to give the drug to 500 people with high blood pressure, and not give the drug to another 500 people with high blood pressure, and see how many in both groups experience lower blood pressure levels. Which is the better way to test this drug?

1. All 1000 get the drug
2. <span id="highlight">500 get the drug, 500 don’t</span>

## Comments on Transition to Ch 3

Now that we're entering Chapter 3, we will be revisiting a lot of the ideas we saw in Chapters 1 and 2, but adding mathematical formality to things. 

All the spots in the previous weeks where we said "we'll revisit this later" will be coming up in the next little while.

## Results from the GSS

The General Social Survey (GSS) collects information and keep a historical record of the concerns, experiences, attitudes, and practices of residents of the United States. Since 1972, the GSS has been monitoring societal change and studying the growing complexity of American society. Canada has been running a similar survey since 1985. 

The GSS asks the question from the previous slide. Below is the distribution of responses from the 2010 survey:

All 1000 get the drug                99
---------------------                ---
500 get the drug, 500 don't          571
Total                                670

<span id="footnote">http://www.statcan.gc.ca/pub/89f0115x/89f0115x2013001-eng.htm</span>

## Parameter and Point Estimation

We would like to estimate the proportion of all Americans who have good intuition about experimental design, i.e., would answer “500 get the drug, 500 don't”? What are the parameter of interest and the point estimate?

## Parameter and Point Estimation

We would like to estimate the proportion of all Americans who have good intuition about experimental design, i.e., would answer “500 get the drug, 500 don't”? What are the parameter of interest and the point estimate?

<span id="highlight">Parameter of Interest</span>: Proportion of **all** Americans who have good intuition about experimental design

**p**: a population proportion

## Parameter and Point Estimation

We would like to estimate the proportion of all Americans who have good intuition about experimental design, i.e., would answer “500 get the drug, 500 don't”? What are the parameter of interest and the point estimate?

<span id="highlight">Parameter of Interest</span>: Proportion of **all** Americans who have good intuition about experimental design

**p**: a population proportion

<span id="highlight">Point Estimate</span>: proportion of **sampled** Americans who have good intuition about experimental design.

**$\hat{p}$**: a sample proportion

## Inference on a Proportion

What percent of all Americans have good intuition about experimental design, i.e., would answer "500 get the drug
500 don't"?

<span id="highlight">We can answer this question using a confidence interval, which (from Ch. 2) we know is always of the form 
<center>
point estimate $\pm$ ME
</center>
where
<center>
$\text{ME} = z^* \times \text{SE}$.
</center>

So what is the SE of our point estimate, $\text{SE}_{\hat{p}}$?

## New Formula: SE of a Point Estimate $\hat{p}$

When we have a **sample proportion**, the standard error has a known formula:

$$
\text{SE}_{\hat{p}} = \sqrt{\frac{p(1-p)}{n}}
$$

What are $p$ and $n$?

1. $n$ is the number of samples (it's a **sample proportion**)
2. $p$ is the true underlying population proportion ...

But we don't know $p$!

We "cheat" here, and replace $p$ with $\hat{p}$. It mostly works.

## Sample Proportions are Almost Normally Distributed

Remember the Central Limit Theorem (CLT) from Ch.2. 

Sample proportions will be nearly normally distributed with mean equal to the population mean, $p$, and standard error equal to $\text{SE}_\hat{p}$ from the last slide. We can write this formally.

$$
\hat{p} \sim \mathcal{N} \left( \text{mean} = p, \text{SE} = \sqrt{\frac{p(1-p)}{n}} \right)
$$

But, of course, this is only true under certain conditions ... <span id="highlight">any guesses?</span>

## Sample Proportions are Almost Normally Distributed

Remember the Central Limit Theorem (CLT) from Ch.2. 

Sample proportions will be nearly normally distributed with mean equal to the population mean, $p$, and standard error equal to $\text{SE}_\hat{p}$ from the last slide. We can write this formally.

$$
\hat{p} \sim \mathcal{N} \left( \text{mean} = p, \text{SE} = \sqrt{\frac{p(1-p)}{n}} \right)
$$

But, of course, this is only true under certain conditions ... <span id="highlight">any guesses?</span>

<span id="highlight">The requirements of the CLT! Independent observations, and "enough" samples</span>

## Rule of Thumb for Proportions

There is a rule of thumb for what "enough samples" means for a sample proportion inference:

1. At least 10 success cases
2. At least 10 failure cases

If you do not have the above, the CLT may not be a good approximation. 

## Linking Back to Ch. 2

Remember what we did in our case studies in Ch. 2. We can **always** use the simulation (permutation test) method, and it will give good results. But it's a lot of work.

So we introduced the normal method and the normal approximation. Because it can be done without a computer, and is sometimes faster.

**But**: it doesn't always work! Keep this in mind.

## Back to the GSS Question

<span id="highlight">The GSS found that 571 out of 670 (85%) of Americans answered the question on experimental design correctly. Estimate (using a 95% confidence interval) the proportion of all Americans who have good intuition about experimental design?</span>

## Back to the GSS Question

<span id="highlight">The GSS found that 571 out of 670 (85%) of Americans answered the question on experimental design correctly. Estimate (using a 95% confidence interval) the proportion of all Americans who have good intuition about experimental design?</span>

Given: 

* $n = 670$
* $\hat{p} = 0.852$

Check the conditions!

## Back to the GSS Question

<span id="highlight">The GSS found that 571 out of 670 (85%) of Americans answered the question on experimental design correctly. Estimate (using a 95% confidence interval) the proportion of all Americans who have good intuition about experimental design?</span>

Given: 

* $n = 670$
* $\hat{p} = 0.852$

Check the conditions!

1. **Independence**: The GSS is sampled randomly, and the population is much larger than the sample, so we can assume the responses are random.

## Back to the GSS Question

<span id="highlight">The GSS found that 571 out of 670 (85%) of Americans answered the question on experimental design correctly. Estimate (using a 95% confidence interval) the proportion of all Americans who have good intuition about experimental design?</span>

Given: 

* $n = 670$
* $\hat{p} = 0.852$

Check the conditions!

1. **Independence**: The GSS is sampled randomly, and the population is much larger than the sample, so we can assume the responses are random.
2. **Enough Samples**: 571 people answered correctly (success) and 99 answered incorrectly (failure). Both numbers are greater than 10.

## Practice
We are given $n=670$, $\hat{p} =0.852$, and we know that 
$$
\text{SE}_\hat{p} = \sqrt{\frac{p(1-p)}{n}}.
$$
Which of the following is the correct calculation of the 95% confidence interval?

1. $0.852 \pm 1.96 \times \sqrt{\frac{0.85(0.15)}{670}}$
2. $0.852 \pm 1.65 \times \sqrt{\frac{0.85(0.15)}{670}}$
3. $0.852 \pm 1.96 \times \frac{0.85(0.15)}{\sqrt{670}}$
4. $571 \pm 1.96 \times \frac{571(99)}{670}$

## Practice
We are given $n=670$, $\hat{p} =0.852$, and we know that 
$$
\text{SE}_\hat{p} = \sqrt{\frac{p(1-p)}{n}}.
$$
Which of the following is the correct calculation of the 95% confidence interval?

1. <span id="highlight">$0.852 \pm 1.96 \times \sqrt{\frac{0.85(0.15)}{670}} = (0.825, 0.879)$</span>
2. $0.852 \pm 1.65 \times \sqrt{\frac{0.85(0.15)}{670}}$
3. $0.852 \pm 1.96 \times \frac{0.85(0.15)}{\sqrt{670}}$
4. $571 \pm 1.96 \times \frac{571(99)}{670}$

## New Idea: Choosing a Sample Size

We now have the formulas which allow us to compute confidence intervals using the normal approximation for sample proportions.

<span id="highlight">How many people should you sample in order to cut the margin of error of a 95% confidence interval down to 1%?</span>

## New Idea: Choosing a Sample Size

We now have the formulas which allow us to compute confidence intervals using the normal approximation for sample proportions.

<span id="highlight">How many people should you sample in order to cut the margin of error of a 95% confidence interval down to 1%?</span>

$$
\text{ME} = z^* \times \text{SE}
$$


## New Idea: Choosing a Sample Size

We now have the formulas which allow us to compute confidence intervals using the normal approximation for sample proportions.

<span id="highlight">How many people should you sample in order to cut the margin of error of a 95% confidence interval down to 1%?</span>

$$
\begin{split}
\text{ME} &= z^* \times \text{SE} \\
0.01 &\geq 1.96 \times \sqrt{\frac{0.85(0.15)}{n}}
\end{split}
$$

(since 1% = 0.01, 95% CI means use $z^* = 1.96$, and we're still interested in the GSS question)

## New Idea: Choosing a Sample Size

We now have the formulas which allow us to compute confidence intervals using the normal approximation for sample proportions.

<span id="highlight">How many people should you sample in order to cut the margin of error of a 95% confidence interval down to 1%?</span>

$$
\begin{split}
\text{ME} &= z^* \times \text{SE} \\
0.01 &\geq 1.96 \times \sqrt{\frac{0.85(0.15)}{n}} \\
(0.01)^2 &\geq 1.96^2 \times \frac{0.85(0.15)}{n} \qquad \; \text{square both sides} \\
\end{split}
$$

## New Idea: Choosing a Sample Size

We now have the formulas which allow us to compute confidence intervals using the normal approximation for sample proportions.

<span id="highlight">How many people should you sample in order to cut the margin of error of a 95% confidence interval down to 1%?</span>

$$
\begin{split}
\text{ME} &= z^* \times \text{SE} \\
0.01 &\geq 1.96 \times \sqrt{\frac{0.85(0.15)}{n}} \\
(0.01)^2 &\geq 1.96^2 \times \frac{0.85(0.15)}{n} \qquad \; \text{square both sides} \\
n (0.01)^2 &\geq 1.96^2 \times 0.85(0.15) \qquad \; \text{cross-multiply the denominator}\\
n &\geq 1.96^2 \times \frac{0.85(0.15)}{0.01^2}
\end{split}
$$



## New Idea: Choosing a Sample Size

We now have the formulas which allow us to compute confidence intervals using the normal approximation for sample proportions.

<span id="highlight">How many people should you sample in order to cut the margin of error of a 95% confidence interval down to 1%?</span>

$$
\begin{split}
\text{ME} &= z^* \times \text{SE} \\
0.01 &\geq 1.96 \times \sqrt{\frac{0.85(0.15)}{n}} \\
n &\geq 1.96^2 \times \frac{0.85(0.15)}{0.01^2} \\
n &\geq 4898.04
\end{split}
$$

<span id="highlight">Therefore we need at least 4,899 samples to cut the ME of a 95% confidence interval (for this problem) down to 1%.</span>

## A Tricky Bit

You may have noticed that we assumed that the ME for the new sample would be the same as the old sample: we used 0.85 and 0.15 in our formula!

This is common if we are repeating a study. The first study is sometimes called a **pilot study**, and it gives us a rough idea what our $\hat{p}$ might be.

What if we didn't have a pilot study to rely on?

## A Tricky Bit

You may have noticed that we assumed that the ME for the new sample would be the same as the old sample: we used 0.85 and 0.15 in our formula!

This is common if we are repeating a study. The first study is sometimes called a **pilot study**, and it gives us a rough idea what our $\hat{p}$ might be.

What if we didn't have a pilot study to rely on?

<span id="highlight">Use the default $\hat{p} = 0.5$.</span>

## A Tricky Bit

Why would $\hat{p} = 0.50$ be a default?

* If you don't know any better, 50/50 guessing seems reasonable
* Using $\hat{p} = 0.50$ is the most conservative estimate, and gives the highest possible sample size number


(for those who have some calculus and are curious) 0.5 is a maximum because the formula has $\hat{p}(1 - \hat{p})$, which is a quadratic function of $\hat{p}$, which has a local maximum at $\hat{p} = 0.50$.

## Confidence Intervals versus Hypothesis Testing

<div style="text-size:18px; top:-20px;">
We have slightly different **Success-Failure Conditions** (for number of samples required):

* **CI**: at least 10 *observed* successes and failures
* **HT**: at least 10 *expected* successes and failures, under the null assumption

## Confidence Intervals versus Hypothesis Testing

<div style="text-size:18px; top:-20px;">
**Standard Error**

* **CI**: calculated using the sample proportion, $\hat{p}$
$$
\text{SE} = \sqrt{\frac{p(1-p)}{n}}.
$$
* **HT**: calculated using the null hypothesis value, $p_0$
$$
\text{SE} = \sqrt{\frac{p_0(1-p_0}{n}}.
$$
</div>

## Practice

The GSS found that 571 out of 670 (85%) of Americans answered the question on experimental design correctly. Do these data provide convincing evidence that more than 80% of Americans have a good intuition about experimental design?

$$
H_0: p = 0.80 \qquad \; \qquad H_A: p > 0.80
$$
(we use a one-tailed hypothesis test because that is our question: "more than 80%")

## Practice

The GSS found that 571 out of 670 (85%) of Americans answered the question on experimental design correctly. Do these data provide convincing evidence that more than 80% of Americans have a good intuition about experimental design?

$$
\begin{split}
H_0: p = 0.80 \qquad &\; \qquad H_A: p > 0.80\\
\text{SE} = \sqrt{\frac{0.80(0.20)}{670}} &= 0.0154\\
\end{split}
$$

## Practice

The GSS found that 571 out of 670 (85%) of Americans answered the question on experimental design correctly. Do these data provide convincing evidence that more than 80% of Americans have a good intuition about experimental design?

$$
\begin{split}
H_0: p = 0.80 \qquad &\; \qquad H_A: p > 0.80\\
\text{SE} = \sqrt{\frac{0.80(0.20)}{670}} &= 0.0154\\
Z = \frac{0.85 - 0.80}{0.0154} &= 3.25 \\
\end{split}
$$

## Practice (*p*-value)

Compute the *p*-value for this value of $z$.

**Method 1**:
Use R!
```{r}
1 - pnorm(3.25)
```
So the *p*-value is 0.0006.

(Why did we use **1-pnorm()**? Because we want the area to the **right**, and **pnorm()** gives the area to the **left**!)

## Practice (*p*-value)

Compute the *p*-value for this value of $z$.

**Method 2**:
Look up the value in a table

<center>
```{r, out.width = "750px", echo = FALSE}
knitr::include_graphics("fig/fig_3_1_ztable.png")
```
</center>

So the *p*-value is 1-0.9994 = 0.006.

## Practice: Conclusion

Since the *p*-value is low, we reject $H_0$. The data provide convincing evidence that more than 80% of Americans have a good intuition on experimental design.

## Practice

11% of 1,001 Americans responding to a 2006 Gallup survey stated that they have objections to celebrating Halloween on religious grounds. At 95% confidence level, the margin of error for this survey is $\pm$3%. A news piece on this study's findings states: "More than 10% of all Americans have objections on religious grounds to celebrating Halloween." At 95% confidence level, is this news piece's statement justified?

1. Yes
2. No
3. Can’t tell

## Practice

11% of 1,001 Americans responding to a 2006 Gallup survey stated that they have objections to celebrating Halloween on religious grounds. At 95% confidence level, the margin of error for this survey is $\pm$3%. A news piece on this study's findings states: "More than 10% of all Americans have objections on religious grounds to celebrating Halloween." At 95% confidence level, is this news piece's statement justified?

1. Yes
2. <span id="highlight">No</span>
3. Can’t tell

## Recap: Inference for One Proportion

Population parameter $p$, Point Estimate $\hat{p}$


## Recap: Inference for One Proportion

Population parameter $p$, Point Estimate $\hat{p}$

Conditions:

* independence
  * random sample, and less than 10% of population
* at least 10 successes and 10 failures
  * if not, we can't use the normal approximation $\rightarrow$ use randomization/permutation instead
  
## Recap: Inference for One Proportion

Population parameter $p$, Point Estimate $\hat{p}$

Conditions:

* independence
    - random sample, and less than 10% of population
* at least 10 successes and 10 failures
    - if not, we can't use the normal approximation $\rightarrow$ use randomization/permutation instead
* Standard Error (SE) = $\sqrt{\frac{p(1-p)}{n}}$
    - for CI, use $\hat{p}$
    - for HT, use $p_0$
    
<!-- This is Chapter 3.2 in the text, slides by Mine Cetinkaya-Rundel -->

# Difference of Two Proportions

## Melting ice cap
Scientists predict that global warming may have big effects on the polar regions within the next 100 years. One of the possible effects is that the northern ice cap may completely melt. Would this bother you a great deal, some, a little, or not at all if it actually happened?

1. A great deal
2. Some
3. A little
4. Not at all

## Results from the GSS
The GSS asks the same question, below are the distributions of responses from the 2010 GSS as well as from a group of introductory statistics students at Duke University:

              GSS    Duke
---------     ---    ----
A great deal  454      69
Some          124      30
A little       52       4
Not at All     50       2
Total         680     105

## Parameter and Point Estimate

* **Parameter of interest**: the difference between the proportions of **all** Duke students and **all** Americans who would be bothered a great deal by the northern ice cap completely melting.

<center>
$$
  p_{\text{Duke}} - p_\text{USA}
$$
</center>

## Parameter and Point Estimate

* **Parameter of interest**: the difference between the proportions of **all** Duke students and **all** Americans who would be bothered a great deal by the northern ice cap completely melting.

<center>
$$
  p_{\text{Duke}} - p_\text{USA}
$$
</center>

* **Point estimate**: difference between the proportions of **sampled** Duke students and **sampled** Americans who would be bothered a great deal by the northern ice cap completely melting.

<center>
$$
  \hat{p}_{\text{Duke}} - \hat{p}_\text{USA}
$$
</center>

## Inference for comparing proportions

* The details are the same as Ch. 3.1 for a single proportion!
* CI: <span id="highlight">point estimate $\pm$ ME</span>

## Inference for comparing proportions

* The details are the same as Ch. 3.1 for a single proportion!
* CI: <span id="highlight">point estimate $\pm$ ME</span>
* HT: use <span id="highlight">$Z =$ (point estimate - null value)/SE</span> to find the appropriate *p*-value

## Inference for comparing proportions

* The details are the same as Ch. 3.1 for a single proportion!
* CI: <span id="highlight">point estimate $\pm$ ME</span>
* HT: use <span id="highlight">$Z =$ (point estimate - null value)/SE</span> to find the appropriate *p*-value
* We just need the appropriate standard error of the point estimate, $\text{SE}_{\hat{p}_{\text{Duke}} - \hat{p}_\text{USA}}$, which is the only new concept

## Inference for comparing proportions

* The details are the same as Ch. 3.1 for a single proportion!
* CI: <span id="highlight">point estimate $\pm$ ME</span>
* HT: use <span id="highlight">$Z =$ (point estimate - null value)/SE</span> to find the appropriate *p*-value
* We just need the appropriate standard error of the point estimate, $\text{SE}_{\hat{p}_{\text{Duke}} - \hat{p}_\text{USA}}$, which is the only new concept

<span id="highlight">Standard error of the difference between two sample proportions is:
$$
\text{SE}_{\hat{p}_{\text{Duke}} - \hat{p}_\text{USA}} = 
\sqrt{ \frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}
$$

## Conditions for CI for difference of proportions

1. <span id="highlight">Independence within groups</span>
    - the USA group is sampled randomly, and we assume that the Duke group represents a random sample as well


## Conditions for CI for difference of proportions

1. <span id="highlight">Independence within groups</span>
    - the USA group is sampled randomly, and we assume that the Duke group represents a random sample as well
    - $n_\text{Duke} <$ 10% of all Duke students, and 680 < 1% of all Americans

## Conditions for CI for difference of proportions

1. <span id="highlight">Independence within groups</span>
    - the USA group is sampled randomly, and we assume that the Duke group represents a random sample as well
    - $n_\text{Duke} <$ 10% of all Duke students, and 680 < 1% of all Americans
    
We can also assume that the attitudes of Duke students in the sample are independent of each other, and attitudes of the US residents in the sample are independent of each other as well.

## Conditions for CI for difference of proportions

1. <span id="highlight">Independence within groups</span>
    - the USA group is sampled randomly, and we assume that the Duke group represents a random sample as well
    - $n_\text{Duke} <$ 10% of all Duke students, and 680 < 1% of all Americans. <br/>
We can also assume that the attitudes of Duke students in the sample are independent of each other, and attitudes of the US residents in the sample are independent of each other as well.
2. <span id="highlight">Independence between groups</span>
The sampled Duke students and the US residents are independent of each other.

## Conditions for CI for difference of proportions

1. <span id="highlight">Independence within groups</span>
    - the USA group is sampled randomly, and we assume that the Duke group represents a random sample as well
    - $n_\text{Duke} <$ 10% of all Duke students, and 680 < 1% of all Americans. <br/>
We can also assume that the attitudes of Duke students in the sample are independent of each other, and attitudes of the US residents in the sample are independent of each other as well.
2. <span id="highlight">Independence between groups</span>:
The sampled Duke students and the US residents are independent of each other.
3. <span id="highlight">Success/Failure</span>: At least 10 observed successes and 10 observed failures in the two groups.

## Practice

<span id="highlight">Construct a 95% confidence interval for the difference between the proportions of Duke students and Americans who would be bothered "A great deal" by the melting of the northern ice cap, $(p_\text{Duke} - p_\text{US})$.

                  GSS    Duke
---------        ----   -----
A great deal      454      69
Not a great deal  226      36
Total             680     105

## Practice

<span id="highlight">Construct a 95% confidence interval for the difference between the proportions of Duke students and Americans who would be bothered "A great deal" by the melting of the northern ice cap, $(p_\text{Duke} - p_\text{US})$.

                  GSS    Duke
---------        ----   -----
A great deal      454      69
Not a great deal  226      36
Total             680     105
$\hat{p}$         0.657   0.668

## Practice

                  GSS    Duke
---------        ----   -----
A great deal      454      69
Not a great deal  226      36
Total             680     105
$\hat{p}$         0.657   0.668

$$
\begin{split}
\left( \hat{p}_\text{Duke} - \hat{p}_{USA} \right) &\pm \sqrt{ \frac{\hat{p}_\text{Duke}(1 - \hat{p}_\text{Duke})}{n_\text{Duke}} +  \frac{\hat{p}_\text{USA}(1 - \hat{p}_\text{USA})}{n_\text{USA}}} \\
\end{split}
$$

## Practice

$$
\begin{split}
\left( \hat{p}_\text{Duke} - \hat{p}_{USA} \right) &\pm z^* \times \sqrt{ \frac{\hat{p}_\text{Duke}(1 - \hat{p}_\text{Duke})}{n_\text{Duke}} +  \frac{\hat{p}_\text{USA}(1 - \hat{p}_\text{USA})}{n_\text{USA}}} \\
(0.657 - 0.668) &\pm 1.96 \times \sqrt{ \frac{0.657(0.343)}{105} + \frac{0.668(0.332)}{680}} \\
-0.011 &\pm 1.96 \times 0.0497 \\
(-0.108, 0.086).
\end{split}
$$

## Practice
<span id="highlight">Which of the following is the correct set of hypotheses for testing if the proportion of all Duke students who would be bothered a great deal by the melting of the northern ice cap differs from the proportion of all Americans who feel the same?</span>

1. $H_0: p_\text{Duke} = p_\text{USA} \; \text{versus} \;  H_A: p_\text{Duke} \neq p_\text{USA}$
2.  $H_0: \hat{p}_\text{Duke} = \hat{p}_\text{USA} \; \text{versus} \;  H_A: \hat{p}_\text{Duke} \neq \hat{p}_\text{USA}$
3.  $H_0: p_\text{Duke} - p_\text{USA} = 0 \; \text{versus} \;  H_A: p_\text{Duke} - p_\text{USA} \neq 0$
4.  $H_0: \hat{p}_\text{Duke} - p_\text{USA} = 0\; \text{versus} \;  H_A: \hat{p}_\text{Duke} - \hat{p}_\text{USA} \neq 0$

## Practice
<span id="highlight">Which of the following is the correct set of hypotheses for testing if the proportion of all Duke students who would be bothered a great deal by the melting of the northern ice cap differs from the proportion of all Americans who feel the same?</span>

1. <span id="highlight">$H_0: p_\text{Duke} = p_\text{USA} \; \text{versus} \;  H_A: p_\text{Duke} \neq p_\text{USA}$</span>
2.  $H_0: \hat{p}_\text{Duke} = \hat{p}_\text{USA} \; \text{versus} \;  H_A: \hat{p}_\text{Duke} \neq \hat{p}_\text{USA}$
3. <span id="highlight">$H_0: p_\text{Duke} - p_\text{USA} = 0 \; \text{versus} \;  H_A: p_\text{Duke} - p_\text{USA} \neq 0$</span>
4.  $H_0: \hat{p}_\text{Duke} - p_\text{USA} = 0\; \text{versus} \;  H_A: \hat{p}_\text{Duke} - \hat{p}_\text{USA} \neq 0$

<span id="highlight">Both (1) and (3) are correct!</span>

## Connection to One Proportion

Think back to the one proportion approach.

* When constructing a CI for a population proportion, we check if the **observed** numbers of successes and failures are at least 10. <br/>
$$
n\hat{p} > 10 \; \qquad \; n \cdot (1-\hat{p}) \geq 10
$$
* When conducting a HT for a population proportion, we check if the **expected** numbers of successes and failures are at least 10. <br/>
$$
np_0 > 10 \; \qquad \; n \cdot (1-p_0) \geq 10
$$

## Pooled estimate of a proportion

* In the case of comparing two proportions where $H_0: p_1 = p_2$, there isn't a given null value we can use to calculate the **expected** number of successes and failures in each sample.

## Pooled estimate of a proportion

* In the case of comparing two proportions where $H_0: p_1 = p_2$, there isn't a given null value we can use to calculate the **expected** number of successes and failures in each sample.
* Therefore, we need to first find a common (*pooled*) proportion for the two groups, and then use **that** in our analysis

## Pooled estimate of a proportion

* In the case of comparing two proportions where $H_0: p_1 = p_2$, there isn't a given null value we can use to calculate the **expected** number of successes and failures in each sample.
* Therefore, we need to first find a common (*pooled*) proportion for the two groups, and then use **that** in our analysis
* This means finding the total proportion of successes among the total number of observations.

## Pooled estimate of a proportion

* In the case of comparing two proportions where $H_0: p_1 = p_2$, there isn't a given null value we can use to calculate the **expected** number of successes and failures in each sample.
* Therefore, we need to first find a common (*pooled*) proportion for the two groups, and then use **that** in our analysis
* This means finding the total proportion of successes among the total number of observations.

**Pooled estimate of a proportion**
$$
\hat{p} = \frac{\text{# of successes}_1 + \text{# of successes}_2}{n_1 + n_2}
$$

## Practice
<span id="highlight">Calculate the estimated **pooled proportion** of Duke students and Americans who would be bothered a great deal by the melting of the northern ice cap. Which sample proportion ($\hat{p}_\text{Duke} or $\hat{p}_\text{USA}$) is the pooled estimate closer to? Why?

                  GSS    Duke
---------        ----   -----
A great deal      454      69
Not a great deal  226      36
Total             680     105
$\hat{p}$         0.657   0.668

## Practice
<div style="font-size:18px;">
<span id="highlight">Calculate the estimated **pooled proportion** of Duke students and Americans who would be bothered a great deal by the melting of the northern ice cap. Which sample proportion ($\hat{p}_\text{Duke} or $\hat{p}_\text{USA}$) is the pooled estimate closer to? Why?

                  GSS    Duke
---------        ----   -----
A great deal      454      69
Not a great deal  226      36
Total             680     105
$\hat{p}$         0.657   0.668

$$
\hat{p} = \frac{\text{# of successes}_1 + \text{# of successes}_2}{n_1 + n_2}
$$
</div>
## Practice

<div style="font-size:18px;">
                  GSS    Duke
---------        ----   -----
A great deal      454      69
Not a great deal  226      36
Total             680     105
$\hat{p}$         0.657   0.668

$$
\begin{split}
\hat{p} &= \frac{\text{# of successes}_1 + \text{# of successes}_2}{n_1 + n_2}\\
&= \frac{69+454}{105+680} = 0.666.
\end{split}
$$

</div>

## CI versus HT for proportions
<span id="highlight">Do these data suggest that the proportion of all Duke students who would be bothered a great deal by the melting of the northern ice cap differs from the proportion of all Americans who do? Calculate the test statistic, the *p*-value, and interpret your conclusion in the context of the data.</span>

$$
\begin{split}
Z &= \frac{\left( \hat{p}_\text{Duke} - \hat{p}_\text{USA} \right)}{\sqrt{ \frac{\hat{p}(1-\hat{p})}{n_\text{Duke}} + \frac{\hat{p}(1-\hat{p})}{n_\text{USA}} }} \\
&= \frac{(0.657 - 0.668)}{\sqrt{ \frac{0.666\cdot 0.334}{105} + \frac{0.666 \cdot 0.334}{680}}} = \frac{-0.011}{0.0495} = -0.22. \\
p-\text{value} &= 2 \cdot \mathbf{P}\left[ Z < -0.22 \right] = 2 \cdot 0.41 = 0.82.
\end{split}
$$

## Practice (conclusion)

Since our *p*-value is 0.82 (which is not small), we do not have evidence at the 95% confidence level to reject the null hypothesis. We cannot say that the proportions of Duke students and Americans who would be bothered a great deal by the melting of the northern ice cap differs in a meaningful way.

## Recap - Comparing Two Proportions

* Population parameter: $(p_1 - p_2)$, &nbsp; point estimate: ($\hat{p}_1 - \hat{p}_2)$

## Recap - Comparing Two Proportions

* Population parameter: $(p_1 - p_2)$, &nbsp; point estimate: ($\hat{p}_1 - \hat{p}_2)$
* Conditions:
    - independence within groups
        - random sample and 10% condition met for both groups
   
## Recap - Comparing Two Proportions

* Population parameter: $(p_1 - p_2)$, &nbsp; point estimate: ($\hat{p}_1 - \hat{p}_2)$
* Conditions:
    - independence within groups
        - random sample and 10% condition met for both groups
    - independence between groups
    - at least 10 successes and failures in each group
        - if not $\rightarrow$ use randomization (Chapter 6.4)

## Recap - Comparing Two Proportions

* Population parameter: $(p_1 - p_2)$, &nbsp; point estimate: ($\hat{p}_1 - \hat{p}_2)$
* Conditions:
    - independence within groups
        - random sample and 10% condition met for both groups
    - independence between groups
    - at least 10 successes and failures in each group
        - if not $\rightarrow$ use randomization (Chapter 6.4)
       
## Recap - Comparing Two Proportions
* Pooled standard error: $\text{SE}_{\hat{p}_1 - \hat{p}_2} = \sqrt{ \frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}$
    - for CI: use $\hat{p}_1$ and $\hat{p}_2$


## Recap - Comparing Two Proportions
* Pooled standard error: $\text{SE}_{\hat{p}_1 - \hat{p}_2} = \sqrt{ \frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}$
    - for CI: use $\hat{p}_1$ and $\hat{p}_2$
    - for HT:
        - if $H_0: p_1 = p_2$, then use $\hat{p}_\text{pool} = \frac{\text{# suc}_1 + \text{# suc}_2}{n_1 + n_2}$
        - otherwise, use $\hat{p}_1$ and $\hat{p}_2$ (this is rare!)
        
## Reference: standard error calculations (so far!)

<center>
```{r, out.width = "750px", echo = FALSE}
knitr::include_graphics("fig/fig_3_2_recap.png")
```
</center>

We haven't talked much about working with means (versus proportions), but in this case, it is very rare that $\sigma$ is known, so we usually use the sample standard deviation $s$. 

## Note on Working with Proportions

* When working with proportions,
    - if doing a hypothesis test, $p$ comes from the null hypothesis
    - if constructing a confidence interval, use $\hat{p}$ instead
    

<!-- This is Chapter 3.3 in the text, slides by Mine Cetinkaya-Rundel -->

# Chi-Square Test for Goodness-of-Fit

## Weldon's dice

<div style="float:left; display:inline-block; width:70%; font-size:23px;">
* Walter Frank Raphael Weldon (1860-1906), was an English evolutionary biologist and a founder of biometry. He was the joint founding editor of *Biometrika*, with Francis Galton and Karl Pearson.
* In 1894, he rolled 12 dice 26,306 times, and recorded the number of 5s or 6s (which he considered to be a success).
</div>
<div style="float:right; display:inline-block; width:30%;">
```{r, out.width = "200px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_weldon.png")
```
</div>
<div style="font-size:23px;">
*  It was observed that 5s and 6s occurred more often than expected, and Pearson hypothesized that this was probably due to the construction of the dice. Most inexpensive dice have hollowed-out pips, and since opposite sides add to 7, the face with 6 pips is lighter than its opposing face, which has only 1 pip.
</div>
<span id="footnote">Picture from Wikipedia: https://en.wikipedia.org/wiki/Raphael_Weldon. Public domain image from Issue 1 of *Biometrika*.</span>

## Labby's Dice

<div style="display:inline-block; float:left; width:70%;">
* In 2009, Zachariah Labby (University of Chicago) repeated Weldon's experiment using a home-made dice-throwing, pip-counting machine.
* The rolling+imaging process took about 20 seconds per roll.
* Each day there were $\approx 150$ images to process manually
* At this rate, Weldon's experiment was repeated in a little more than six full days
</div>
<div style="display:inline-block; float:right; left:25px; width:25%;">
```{r, out.width = "300px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_labby.png")
```
</div>
<span id="footnote">Image and source: http://galton.uchicago.edu/about/docs/labby09dice.pdf</span>

## Labby's dice (continued)

* Labby did not actually observe the same phenomenon that Weldon observed (higher frequency of 5s and 6s)
* Automation allowed Labby to collect more data than Weldon did in 1894, so instead of recording "successes" and "failures", Labby recorded the individual number of pips on each die.

<center>
```{r, out.width = "400px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_labby_pips.png")
```
</center>

## Expected Counts

Labby rolled 12 dice 26,306 times. If each side is equally likely to come up, how many 1s, 2s, 3s, 4s, 5s, and 6s would he expect to have observed?

1. 1/6
2. 12/6
3. 26,306/6
4. 12 $\cdot$ 26,306 /6

## Expected Counts

Labby rolled 12 dice 26,306 times. If each side is equally likely to come up, how many 1s, 2s, 3s, 4s, 5s, and 6s would he expect to have observed?

1. 1/6
2. 12/6
3. 26,306/6
4. <span id="highlight">12 $\cdot$ 26,306 /6 = 56,612.</span>

## Summarizing Labby's results

The table below shows the observed and expected counts from Labby's experiment.

<center>
```{r, out.width = "400px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_labby_results.png")
```
</center>

## Summarizing Labby's results

The table below shows the observed and expected counts from Labby's experiment.

<center>
```{r, out.width = "400px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_labby_results.png")
```
</center>

<span style="font-colour:blue;">Why are the expected counts the same for all outcomes, but the observed counts are different? At a first glance, does there appear to be an inconsistency between the observed and expected counts?</span>

## Setting the hypothesis

<span style="font-colour:blue;">Do these data provide convincing evidence of an inconsistency between the observed and expected counts?</span>

## Setting the hypothesis

<span style="font-colour:blue;">Do these data provide convincing evidence of an inconsistency between the observed and expected counts?</span>

$H_0$: There is no inconsistency between the observed and expected counts. **The observed counts follow the same distribution as the expected counts**.

## Setting the hypothesis

<span style="font-colour:blue;">Do these data provide convincing evidence of an inconsistency between the observed and expected counts?</span>

$H_0$: There is no inconsistency between the observed and expected counts. **The observed counts follow the same distribution as the expected counts**.

$H_A$: There is an inconsistency between the observed and the expected counts. **The observed counts do not follow the same distribution as the expected counts**. There is a bias in which side comes up on the roll of a die. 

## Bias

Reminder: in statistics, **bias** refers to the tendency of a measurement process to over- or under-estimate the value of a population parameter. 

In the case of the dice-rolling experiment, the bias is the tendency of rolled dice to (potentially) not show all faces the same proportion of time.

## Evaluating the hypotheses

* To evaluate these hypotheses, we quantify how different the observed counts are from the expected counts

## Evaluating the hypotheses

* To evaluate these hypotheses, we quantify how different the observed counts are from the expected counts
* Large deviations from what would be expected based on sampling variation (chance) alone provide strong evidence for the alternative hypothesis.

## Evaluating the hypotheses

* To evaluate these hypotheses, we quantify how different the observed counts are from the expected counts
* Large deviations from what would be expected based on sampling variation (chance) alone provide strong evidence for the alternative hypothesis.
* This is called a **goodness-of-fit** (or sometimes, a **goodness of fit**) test, since we're evaluating how well the observed data fit the expected distribution.

## Anatomy of a test statistic

The general form of a test statistic is
$$
\frac{\text{point estimate} - \text{null value}}{\text{SE of point estimate}}
$$

## Anatomy of a test statistic

The general form of a test statistic is
$$
\frac{\text{point estimate} - \text{null value}}{\text{SE of point estimate}}
$$

This construction is based on

1. Identifying the difference between a point estimate and an expected value if the null hypothesis was true, and
2. Standardizing that difference using the standard error of the point estimate. 

## Anatomy of a test statistic

The general form of a test statistic is
$$
\frac{\text{point estimate} - \text{null value}}{\text{SE of point estimate}}
$$

This construction is based on

1. Identifying the difference between a point estimate and an expected value if the null hypothesis was true, and
2. Standardizing that difference using the standard error of the point estimate. 

These two ideas will help in the construction of an appropriate test statistic for count data.

## Chi-square statistic

When dealing with counts and investigating how far the observed counts are from the expected counts, we use a new test statistic called the chi-square ($\chi^2$) statistic.

$$
\chi^2 = \sum_{i=1}^{k} \frac{(O-E)^2}{E}
$$
where $k$ is the total number of cells.

## Summation Notation

We used **summation notation** on the last slide, denoted as
$$
\sum_{i=1}^{k}
$$
This is a mathematical short-hand, which stands for "take the sum of the term to the right, allowing the index $i$ to vary from $1$ to $k$. In long form, this reads
$$
\frac{(O_1-E_1)^2}{E_1} + \frac{(O_2-E_2)^2}{E_2} + \cdots + \frac{(O_{k-1}-E_{k-1})^2}{E_{k-1}} + \frac{(O_k-E_k)^2}{E_k}
$$

## Calculating the $\chi^2$ statistic

<center>
```{r, out.width = "700px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq1.png")
```
</center>

## Calculating the $\chi^2$ statistic

<center>
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq2.png")
```
</center>

## Calculating the $\chi^2$ statistic

<center>
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq3.png")
```
</center>


## Calculating the $\chi^2$ statistic

<center>
```{r, out.width = "700px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq4.png")
```
</center>

## Calculating the $\chi^2$ statistic

<center>
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq5.png")
```
</center>


## Calculating the $\chi^2$ statistic
<center>
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq6.png")
```
</center>

## Calculating the $\chi^2$ statistic

Find the totals:

<center>
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq7.png")
```
</center>

## Why Square?

Squaring the difference between the observed and expected outcome does two things:

1. Any standardized difference that is squared will now be positive (not randomly positive or negative depending on which is bigger)
2. Difference that already looked unusual will become much larger after being squared.

<span style="font-colour:blue;">When have we seen this before?</span>

## The Chi-square Distribution

* In order to determine if the $\chi^2$ statistic we calculated is considered unusually high or not we need to first describe its distribution.
* The chi-square distribution has just one parameter called degrees of freedom (*df*), which influences the shape, center, and spread of the distribution.

## The Chi-square Distribution

* In order to determine if the $\chi^2$ statistic we calculated is considered unusually high or not we need to first describe its distribution.
* The chi-square distribution has just one parameter called degrees of freedom (*df*), which influences the shape, center, and spread of the distribution.

**Remember**

So far we've only seen one continuous distribution: the **normal distribution**, which is unimodal and symmetric with two parameters - the mean and the standard deviation.

In the following chapters we will see a number of other distributions.

<!-- WARNING: this was a really weird slide, unless you do the book out-of-order. At 3.3, we've only seen the Normal: no T, no F, etc. Those aren't introduced until Chapter 4 / appendix. -->

## Practice
If the following image shows three different $\chi^2$ distributions for different values of *df*, which of the following statements is false?

<center>
```{r, out.width = "400px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chi_dist.png")
```
</center>

As the *df* increases:

1. The center of the $\chi^2$ distribution increases as well.
2. The variability of the $\chi^2$ distribution increases as wel
3. The shape of the $\chi^2$ distribution becomes more skewed (less like a normal)


## Practice
If the following image shows three different $\chi^2$ distributions for different values of *df*, which of the following statements is false?

<center>
```{r, out.width = "400px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chi_dist.png")
```
</center>

As the *df* increases:

1. The center of the $\chi^2$ distribution increases as well.
2. The variability of the $\chi^2$ distribution increases as wel
3. <span id="highlight">The shape of the $\chi^2$ distribution becomes more skewed (less like a normal)</span>

## Finding areas under the $\chi^2$ curve

* Recall that the *p*-value is the tail area: this remains true for the $\chi^2$ distribution

## Finding areas under the $\chi^2$ curve

* Recall that the *p*-value is the tail area: this remains true for the $\chi^2$ distribution
* As before, we can either use technology or a lookup table

## Finding areas under the $\chi^2$ curve

* Recall that the *p*-value is the tail area: this remains true for the $\chi^2$ distribution
* As before, we can either use technology or a lookup table
* This table only provides **upper-tail** values, and is far more restrictive than the normal table we've seen

<center>
```{r, out.width = "600px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_table_show.png")
```
</center>

## Finding areas under the $\chi^2$ curve

Estimate the shaded area under the $\chi^2$ curve with *df*=6.

<center>
```{r, out.width = "750px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_practice1.png")
```
</center>

## Finding areas under the $\chi^2$ curve

Estimate the shaded area under the $\chi^2$ curve with *df*=6.

<center>
```{r, out.width = "750px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_practice2.png")
```
</center>

## Finding areas under the $\chi^2$ curve

Estimate the shaded area under the $\chi^2$ curve with *df*=6.

<center>
```{r, out.width = "750px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_practice3.png")
```
</center>

## Finding areas under the $\chi^2$ curve

Estimate the shaded area under the $\chi^2$ curve with *df*=6.

<center>
```{r, out.width = "750px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_practice4.png")
```
</center>

## Finding areas under the $\chi^2$ curve

Estimate the shaded area (above 17) under the $\chi^2$ curve with *df*=9.

<div style="display:inline-block; width:50%; float: left;">
```{r, out.width = "300px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_practice5.png")
```
</div>
<div style="display:inline-block; width:50%; float:right;>>

1. Between 0.01 and 0.02
2. 0.02
3. Between 0.02 and 0.05
4. 0.05
5. Between 0.05 and 0.10.

</div>

<center>
```{r, out.width = "750px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_practice6.png")
```
</center>

## Finding areas under the $\chi^2$ curve

Estimate the shaded area (above 17) under the $\chi^2$ curve with *df*=9.

<div style="display:inline-block; width:50%; float: left;">
```{r, out.width = "300px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_practice5.png")
```
</div>
<div style="display:inline-block; width:50%; float:right;>>

1. Between 0.01 and 0.02
2. 0.02
3. <span id="highlight">Between 0.02 and 0.05</span>
4. 0.05
5. Between 0.05 and 0.10.

</div>

<center>
```{r, out.width = "750px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_practice7.png")
```
</center>

## Finding areas under the $\chi^2$ curve

Estimate the shaded area (above 30) under the $\chi^2$ curve with *df*=10.

<div style="display:inline-block; width:50%; float: left;">
```{r, out.width = "300px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_practice8.png")
```
</div>
<div style="display:inline-block; width:50%; float:right;>>

1. greater than 0.3
2. between 0.005 and 0.001
3. less than 0.001
4. greater than 0.001
5. cannot tell using this table
</div>

<center>
```{r, out.width = "750px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_practice9.png")
```
</center>

## Finding areas under the $\chi^2$ curve

Estimate the shaded area (above 30) under the $\chi^2$ curve with *df*=10.

<div style="display:inline-block; width:50%; float: left;">
```{r, out.width = "300px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_practice8.png")
```
</div>
<div style="display:inline-block; width:50%; float:right;>>

1. greater than 0.3
2. between 0.005 and 0.001
3. <span id="highlight">less than 0.001</span>
4. greater than 0.001
5. cannot tell using this table

</div>

<center>
```{r, out.width = "850px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_practice10.png")
```
</center>

## Finding the tail areas using computation

* While probability tables are very helpful in understanding how probability distributions work, and provide quick reference when computational resources are not available, they are somewhat archaic.
* Using R:
```{r}
pchisq(q = 30, df = 10, lower.tail = FALSE)
```
* Can also use web applets (like: http://bitly.com/dist_calc) or fancy (TI-83+) calculators

## Back to Labby's dice

* The research question was: Do these data provide convincing evidence of an inconsistency between the observed and expected counts?

## Back to Labby's dice

* The research question was: Do these data provide convincing evidence of an inconsistency between the observed and expected counts?
* The hypotheses were:
    - $H_0$: There is no inconsistency between the observed and the expected counts. The observed counts follow the same distribution as the expected counts.
    - $H_A$: There is an inconsistency between the observed and the expected counts. The observed counts **do not** follow the same distribution as the expected counts. There is a bias in which side comes up on the roll of a die.

## Back to Labby's dice

* The research question was: Do these data provide convincing evidence of an inconsistency between the observed and expected counts?
* The hypotheses were:
    - $H_0$: There is no inconsistency between the observed and the expected counts. The observed counts follow the same distribution as the expected counts.
    - $H_A$: There is an inconsistency between the observed and the expected counts. The observed counts **do not** follow the same distribution as the expected counts. There is a bias in which side comes up on the roll of a die.
* We had calculated a test statistic of $\chi^2 = 24.73$.
* All we need is the *df* and we can calculate the tail area (the *p*-value) and make a decision on the hypothesis

## Degrees of Freedom (*df*) for a goodness of fit test

* When conducting a goodness of fit test to evaluate how well the observed data follow an expected distribution, the degrees of freedom are calculated as the number of cells ($k$) minus 1.

$$
df = k - 1
$$

## Degrees of Freedom (*df*) for a goodness of fit test

* When conducting a goodness of fit test to evaluate how well the observed data follow an expected distribution, the degrees of freedom are calculated as the number of cells ($k$) minus 1.
$$
df = k - 1
$$

## Degrees of Freedom (*df*) for a goodness of fit test

* When conducting a goodness of fit test to evaluate how well the observed data follow an expected distribution, the degrees of freedom are calculated as the number of cells ($k$) minus 1.
$$
df = k - 1
$$
* For disc outcomes, $k=6$, therefore
$$
df = 6-1 = 5
$$

## Finding the *p*-value for a $\chi^2$ test

The *p*-value for a $\chi^2$ test is defined as the **tail area above the calculated test statistic**.

* Using R:
```{r}
pchisq(q = 24.73, df = 5, lower.tail = FALSE)
```

## Conclusion of the hypothesis test

<span style="font-colour:blue;">We calculated a *p*-value less than 0.001. At 95% significance level, what is the conclusion of the hypothesis test?</span>

1. Reject $H_0$, the data provide convincing evidence that the dice are fair.
2. Reject $H_0$, the data provide convincing evidence that the dice are biased.
3. Fail to reject $H_0$, the data provide convincing evidence that the dice are fair.
4. Fail to reject $H_0$, the data provide convincing evidence that the dice are biased.

## Conclusion of the hypothesis test

<span style="font-colour:blue;">We calculated a *p*-value less than 0.001. At 95% significance level, what is the conclusion of the hypothesis test?</span>

1. Reject $H_0$, the data provide convincing evidence that the dice are fair.
2. <span id="highlight">Reject $H_0$, the data provide convincing evidence that the dice are biased.</span>
3. Fail to reject $H_0$, the data provide convincing evidence that the dice are fair.
4. Fail to reject $H_0$, the data provide convincing evidence that the dice are biased.

## Turns out ...
* The 1-6 axis is consistently shorter than the other two (2-5 and 3-4), thereby supporting the hypothesis that the faces with one and six pips are larger than the other faces.
* Pearson's claim that 5s and 6s appear more often due to the carved-out pips is not supported by these data.
* Dice used in casinos have flush faces, where the pips are filled in with a plastic of the same density as the surrounding material and are precisely balanced.

<div style="display:inline-block; float:left; width:50%;">
```{r, out.width = "350px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_dice1.jpg")
```
</div>
<div style="display:inline-block; float:right; width:50%;">
```{r, out.width = "350px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_dice2.jpg")
```
</div>

## Recap: *p*-value for a $\chi^2$ test

* The *p*-value for a $\chi^2$ test is defined as the tail area **above** the calculated test statistic
* This is because the test statistic is always positive, and a higher test statistic means a stronger deviation from the null hypothesis

<center>
```{r, out.width = "550px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_chisq_pval.png")
```
</center>

## What about Conditions?

1. **Independence**: each case that contributes to a count in the table must be independent of all other cases in the table.


## What about Conditions?

1. **Independence**: each case that contributes to a count in the table must be independent of all other cases in the table.
2. **Sample size**: each particular scenario (cell) must have at least 5 *expected* cases

## What about Conditions?

1. **Independence**: each case that contributes to a count in the table must be independent of all other cases in the table.
2. **Sample size**: each particular scenario (cell) must have at least 5 *expected* cases
3. **df>1**: the number of degrees of freedom must be greater than 1

Failing to check conditions may unintentionally affect the test's error rate.

## 2009 Iran Election
There was lots of talk of election fraud in the 2009 Iran election. We'll compare the data from a poll conducted before the election (observed data) to the reported votes in the election to see if the two follow the same distribution.

<center>
```{r, out.width = "750px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_iran1.png")
```
</center>

## 2009 Iran Election
There was lots of talk of election fraud in the 2009 Iran election. We'll compare the data from a poll conducted before the election (observed data) to the reported votes in the election to see if the two follow the same distribution.

<center>
```{r, out.width = "750px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_iran2.png")
```
</center>

## Hypotheses

<span style="font-colour:blue;">What are the hypotheses if the distributions of reported and polled votes are different?</span>

## Hypotheses

<span style="font-colour:blue;">What are the hypotheses if the distributions of reported and polled votes are different?</span>

$H_0$: The observed counts from the poll follow the same distribution as the reported votes.

$H_A$: The observed counts from the poll do not follow the same distribution as the reported votes.

## Calculation of the test statistic

<center>
```{r, out.width = "950px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_iran3.png")
```
</center>

## Calculation of the test statistic

<center>
```{r, out.width = "950px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_iran3.png")
```
</center>

$$
\begin{split}
\frac{(O_1 - E_1)^2}{E_1} = \frac{(338-319)^2}{319} = 1.13 \\
\end{split}
$$

## Calculation of the test statistic

<center>
```{r, out.width = "950px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_iran3.png")
```
</center>

$$
\begin{split}
\frac{(O_1 - E_1)^2}{E_1} = \frac{(338-319)^2}{319} = 1.13 \\
\frac{(O_2 - E_2)^2}{E_2} = \frac{(136-172)^2}{172} = 7.53 \\
\end{split}
$$

## Calculation of the test statistic

<center>
```{r, out.width = "950px", echo = FALSE}
knitr::include_graphics("fig/fig_3_3_iran3.png")
```
</center>

$$
\begin{split}
\frac{(O_1 - E_1)^2}{E_1} = \frac{(338-319)^2}{319} = 1.13 \\
\frac{(O_2 - E_2)^2}{E_2} = \frac{(136-172)^2}{172} = 7.53 \\
\frac{(O_3 - E_3)^2}{E_3} = \frac{(30-13)^2}{13} = 22.23 \\
\end{split}
$$

## Calculation of the test statistic

Then:
$$
\chi^2_{3-1=2} = 1.13 + 7.53 + 22.23 = 30.89
$$

So what is the *p*-value?

```{r}
pchisq(q = 30.89, df = 2, lower.tail = FALSE)
```


## Conclusion

<span style="font-color:blue;">Based on these calculations, what is the conclusion of the hypothesis test?</span>

1. *p*-value is low, $H_0$ is rejected. The observed counts from the poll do not follow the same distribution as the reported votes.
2. *p*-value is high, $H_0$ is not rejected. The observed counts from the poll follow the same distribution as the reported votes.
3. *p*-value is low, $H_0$ is rejected. The observed counts from the poll follow the same distribution as the reported votes
4. *p*-value is low, $H_0$ is not rejected. The observed counts from the poll do not follow the same distribution as the reported votes.

## Conclusion

Based on these calculations, what is the conclusion of the hypothesis test?

1. <span id="highlight";>*p*-value is low, $H_0$ is rejected. The observed counts from the poll do not follow the same distribution as the reported votes.</span>
2. *p*-value is high, $H_0$ is not rejected. The observed counts from the poll follow the same distribution as the reported votes.
3. *p*-value is low, $H_0$ is rejected. The observed counts from the poll follow the same distribution as the reported votes
4. *p*-value is low, $H_0$ is not rejected. The observed counts from the poll do not follow the same distribution as the reported votes.

<!-- This is Chapter 3.4 in the text, slides by Mine Cetinkaya-Rundel -->

# Chi-Square ($\chi^2$) Test of Independence

## Popular Kids

In the dataset popular, students in grade 4-6 were asked whether good grades, athletic ability, or popularity was most important to them. A two-way table separating students by grade and by choice of their most important factor is shown below. Do these data provide evidence to suggest that goals vary by grade?

<center>
```{r, out.width = "750px", echo = FALSE}
knitr::include_graphics("fig/fig_3_4_popular_table.png")
```
</center>

## Chi-square test of independence

* The hypotheses are:
    - $H_0$: grade and goals are independent. Goals do not vary by grade.
    - $H_A$: grade and goals are dependent. Goals vary by grade.

## Chi-square test of independence

* The hypotheses are:
    - $H_0$: grade and goals are independent. Goals do not vary by grade.
    - $H_A$: grade and goals are dependent. Goals vary by grade.
* The test statistic is calculated as
$$
\chi^2_{df} = \sum_{i=1}^{k} \frac{(O_i-E_i)^2}{E_i} \; \text{where} \; df = (R-1) \cdot (C-1)
$$
with $k$ the number of total cells, $R$ the number of rows, and $C$ the number of columns. 


## Notes

* We calculate the **degrees of freedom**, $df$, differently for one- and two-way tables. 
* The *p*-value for this hypothesis test is again the area under the null distribution (in this case, a $\chi^2_{df}$), to the right of the calculated test statistic.

## So what are $O_i$ and $E_i$?

The terms of the summation are $O_i$ and $E_i$. These are the **observed** and **expected** values of each cell of the table. The observed is easy: it's what is listed in the table!

The **expected** takes a bit of work. We have a formula again:
$$
E_i = \frac{(\text{row total}) \cdot (\text{column total})}{\text{table total}}.
$$

<center>
```{r, out.width = "650px", echo = FALSE}
knitr::include_graphics("fig/fig_3_4_popular_table_totals.png")
```
</center>

## The expected quantities
<center>
```{r, out.width = "650px", echo = FALSE}
knitr::include_graphics("fig/fig_3_4_popular_table_totals.png")
```
</center>

We compute:

* $E_{\text{row 1, col 1}} = \frac{119 \cdot 247}{478} = 61$.

## The expected quantities
<center>
```{r, out.width = "650px", echo = FALSE}
knitr::include_graphics("fig/fig_3_4_popular_table_totals.png")
```
</center>

We compute:

* $E_{\text{row 1, col 1}} = \frac{119 \cdot 247}{478} = 61$.
* $E_{\text{row 1, col 2}} = \frac{119 \cdot 141}{478} = 35$.

## Practice
<center>
```{r, out.width = "650px", echo = FALSE}
knitr::include_graphics("fig/fig_3_4_popular_table_practice.png")
```
</center>
What is the expected count, $E_{\text{row 2, col 2}}$ for the highlighted (in red) cell?

1. $\frac{176 \cdot 141}{478}$
2. $\frac{119 \cdot 141}{478}$
3. $\frac{176 \cdot 247}{478}$
4. $\frac{176 \cdot 478}{478}$

## Practice
<center>
```{r, out.width = "650px", echo = FALSE}
knitr::include_graphics("fig/fig_3_4_popular_table_practice.png")
```
</center>
What is the expected count, $E_{\text{row 2, col 2}}$ for the highlighted (in red) cell?

1. <span id="highlight">$\frac{176 \cdot 141}{478} = 52$</span>
2. $\frac{119 \cdot 141}{478}$
3. $\frac{176 \cdot 247}{478}$
4. $\frac{176 \cdot 478}{478}$

## Calculating the test statistic

Go through each cell of the table, and compute the expected value for that cell using the formula from the previous slides.

<center>
```{r, out.width = "650px", echo = FALSE}
knitr::include_graphics("fig/fig_3_4_popular_expected.png")
```
</center>

## Calculating the test statistic

Go through each cell of the table, and compute the expected value for that cell using the formula from the previous slides.

<center>
```{r, out.width = "650px", echo = FALSE}
knitr::include_graphics("fig/fig_3_4_popular_expected.png")
```
</center>

Then compute the test statistic:
$$
\begin{split}
\chi^2_{df} &= \frac{(63-61)^2}{61} + \frac{(31-35)^2}{35} + \cdots + \frac{(32-34)^2}{34} = 1.3121.\\
df &= (R-1) \cdot (C-1) = (3-1) \cdot (3-1) = 2 \cdot 2 = 4.
\end{split}
$$

## Finding the *p*-value

Just as in the previous examples of *p*-values, there are two easy ways to find a *p*-value for a $\chi^2$ test:

* Use R, or other software packages
* Use a table (back of the book, or internet)

<center>
```{r, out.width = "450px", echo = FALSE}
knitr::include_graphics("fig/fig_3_4_chisquare.png")
```
</center>

(this is what a $\chi^2$ distribution looks like)

## Finding the *p*-value

**Method 1**: use R

```{r}
pchisq(q = 1.3121, df = 4, lower.tail = FALSE)
```

Since this *p*-value is (very!) not small, we do not have evidence to reject the null hypothesis, so we cannot conclude that goals vary by grade.

## Finding the *p*-value

**Method 2**: use a lookup table
<div style="font-size:20px;">
<center>
```{r, out.width = "450px", echo = FALSE}
knitr::include_graphics("fig/fig_3_4_chisquare_table.png")
```
</center>

This table is a bit trickier to read: what we do is look at the **row** that corresponds to our *df*, then try to find our test statistic of 1.3121 on the table. Do you see it?
</div>

## Practice

From using the table, what is the correct *p*-value for this hypothesis test?

<div style="float:left; display:inline-block; width:40%;">
1. More than 0.3
2. Between 0.3 and 0.2
3. Between 0.2 and 0.1
4. Between 0.1 and 0.05
5. Less than 0.001
</div>
<div style="display:inline-block; width:60%;">
```{r, out.width = "500px", echo = FALSE}
knitr::include_graphics("fig/fig_3_4_chisquare_table.png")
```
</div>

## Practice

From using the table, what is the correct *p*-value for this hypothesis test?

<div style="float:left; display:inline-block; width:40%;">
1. <span id="highlight">More than 0.3</span>
2. Between 0.3 and 0.2
3. Between 0.2 and 0.1
4. Between 0.1 and 0.05
5. Less than 0.001
</div>
<div style="display:inline-block; width:60%;">
```{r, out.width = "500px", echo = FALSE}
knitr::include_graphics("fig/fig_3_4_chisquare_table.png")
```
</div>

## Conclusion

What does the data suggest about goals and grade-level? Remember:

* $H_0$: grade and goals are independent. Goals do not vary by grade.
* $H_A$: grade and goals are dependent. Goals vary by grade.

Since we have *p*>0.3, we do not have a small *p*, and we cannot reject the null. Thus, we fail to reject the null hypothesis: the data do not provide convincing evidence that grade and goals are dependent. It does not appear that goals vary by grade.
